{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11547066,"sourceType":"datasetVersion","datasetId":7241323}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n### End-to-End Fine-Tuning on the ISOT Fake News Dataset\n\nFine-tuning a pre-trained transformer model for binary classification (Real vs. Fake News) using Hugging Face `transformers`.\n\n---\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we fine-tune a pre-trained transformer model to perform fake news detection.  \nUsing the ISOT Fake News Dataset, we demonstrate the full end-to-end process, including data loading, preprocessing, tokenization, model training, evaluation, and saving the fine-tuned model.  \nWe leverage the Hugging Face `transformers` library to simplify the workflow and take advantage of powerful state-of-the-art language models.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies\n\nThe following libraries are required for this notebook:\n- `transformers` (for model and training pipeline)\n- `datasets` (for easy dataset handling)\n- `tensorboard` (for training visualization)\n\nIf you are running this notebook for the first time, you may need to install them by uncommenting the commands below.\n","metadata":{}},{"cell_type":"code","source":"#!pip install -q transformers \n#!pip install -q datasets \n#!pip install tensorboard  #TensorBoard is a great option to visualize the training progress in real-time.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:27.100548Z","iopub.execute_input":"2025-04-28T09:46:27.100739Z","iopub.status.idle":"2025-04-28T09:46:27.104875Z","shell.execute_reply.started":"2025-04-28T09:46:27.100722Z","shell.execute_reply":"2025-04-28T09:46:27.104165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup and Configuration\n\nWe import necessary libraries and set up the device configuration for training.","metadata":{}},{"cell_type":"markdown","source":"### Libraries\n\nWe import all the necessary libraries for model training, evaluation, and logging.\n","metadata":{}},{"cell_type":"code","source":"# Silence warnings and reduce logging noise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Silence logging output from the Hugging Face transformers library (which can be verbose)\n#from transformers.utils import logging\n#logging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:27.119617Z","iopub.execute_input":"2025-04-28T09:46:27.119850Z","iopub.status.idle":"2025-04-28T09:46:27.127054Z","shell.execute_reply.started":"2025-04-28T09:46:27.119829Z","shell.execute_reply":"2025-04-28T09:46:27.126388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Libraries\nfrom transformers import Trainer, TrainingArguments, pipeline,\\\n                         DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom datasets import Dataset\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\nimport os\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:27.128297Z","iopub.execute_input":"2025-04-28T09:46:27.128528Z","iopub.status.idle":"2025-04-28T09:46:53.944144Z","shell.execute_reply.started":"2025-04-28T09:46:27.128513Z","shell.execute_reply":"2025-04-28T09:46:53.943398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Paths and Directory Setup\n\nDefine project paths for saving results, logs, and trained models.\n","metadata":{}},{"cell_type":"code","source":"# paths\nPROJECT_ROOT = \"/kaggle/working\"\nDATA_ROOT = \"/kaggle/input/fake-news-dataset-for-llm-fine-tuning/fake_news_detection_Kaggle\"\n\nRESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\nLOGS_DIR = os.path.join(PROJECT_ROOT, \"logs\")\nMODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\", \"fine_tuned_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:53.944909Z","iopub.execute_input":"2025-04-28T09:46:53.945428Z","iopub.status.idle":"2025-04-28T09:46:53.949601Z","shell.execute_reply.started":"2025-04-28T09:46:53.945406Z","shell.execute_reply":"2025-04-28T09:46:53.948801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Quick Check: Using a Pre-trained Model\n\nWe first load a pre-trained DistilBERT model fine-tuned for sentiment analysis to quickly test the text classification workflow.  \n*Note: This model is only used as a placeholder and will later be replaced by our fine-tuned model.*\n","metadata":{}},{"cell_type":"code","source":"#---------------------------------------------------\n# Load the pre-trained model from Hugging Face\n# This will download on first run and cache locally\nclassifier = pipeline(\"text-classification\", \\\n             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n#---------------------------------------------------\ndef predict_with_llm(text):\n    \"\"\"\n    Predict whether the input text is FAKE or REAL using a pre-trained LLM.\n    Note: This model is fine-tuned for sentiment analysis, so this is just a placeholder.\n    \"\"\"\n    result = classifier(text)[0]\n    label = result['label']  # 'POSITIVE' or 'NEGATIVE'\n\n    # TEMPORARY MAPPING\n    if label == 'NEGATIVE':\n        return \"FAKE\"\n    else:\n        return \"REAL\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:53.950307Z","iopub.execute_input":"2025-04-28T09:46:53.950531Z","iopub.status.idle":"2025-04-28T09:46:57.100657Z","shell.execute_reply.started":"2025-04-28T09:46:53.950513Z","shell.execute_reply":"2025-04-28T09:46:57.099907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Preparation\n\nWe load the ISOT Fake News dataset, assign labels (1 for REAL, 0 for FAKE), and combine the true and fake news articles into a single DataFrame.  \nThe dataset is then shuffled, converted to a Hugging Face `Dataset`, and split into training and test sets (80% train / 20% test).\n","metadata":{}},{"cell_type":"code","source":"def prepare_dataset():\n    # Load both parts of ISOT dataset\n    true_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"True.csv\"))\n    fake_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"Fake.csv\"))\n\n    # Add labels\n    true_df[\"label\"] = 1  # REAL\n    fake_df[\"label\"] = 0  # FAKE\n\n    # Keep only the 'text' and 'label' columns\n    true_df = true_df[[\"text\", \"label\"]]\n    fake_df = fake_df[[\"text\", \"label\"]]\n\n    # Combine and shuffle\n    df = pd.concat([true_df, fake_df], ignore_index=True)\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Convert to Hugging Face Dataset and split\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.2)\n\n    return dataset  # A DatasetDict with 'train' and 'test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:57.102801Z","iopub.execute_input":"2025-04-28T09:46:57.102999Z","iopub.status.idle":"2025-04-28T09:46:57.108234Z","shell.execute_reply.started":"2025-04-28T09:46:57.102983Z","shell.execute_reply":"2025-04-28T09:46:57.107397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization\n\nWe use the pre-trained DistilBERT tokenizer from Hugging Face to tokenize the text data.  \nEach text entry is tokenized with truncation and padding to a maximum length of 512 tokens.  \nThe tokenized dataset is then formatted for PyTorch compatibility, making it ready for model training.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset(dataset):\n    '''\n    Load the tokenizer from Hugging Face.\n    Define a preprocessing function that:\n    Tokenize th|e text field.\n    Applly truncation and padding up to 512 tokens.\n    Appliy this to the full dataset using map.\n    Set the format of the returned dataset for PyTorch\n    '''\n    tokenizer = DistilBertTokenizerFast.from_pretrained(\\\n                 'distilbert-base-uncased')\n    def tokenize_fn(example):\n        return tokenizer(\n            example['text'], \n            padding='max_length', \n            truncation=True, \n            max_length=512\n        )\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    # This part is essential for model training with PyTorch\n    tokenized_dataset.set_format(\n        type='torch',\n        columns=['input_ids', 'attention_mask', 'label']\n    )\n    return tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:57.109378Z","iopub.execute_input":"2025-04-28T09:46:57.109662Z","iopub.status.idle":"2025-04-28T09:46:57.159737Z","shell.execute_reply.started":"2025-04-28T09:46:57.109639Z","shell.execute_reply":"2025-04-28T09:46:57.159049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Prepare the dataset\ndataset = prepare_dataset()  # This returns a Hugging Face DatasetDict\n#print(dataset)  # Should now show DatasetDict with train/test\n\n# Step 2: Tokenize the dataset\ntokenized_dataset = tokenize_dataset(dataset)\n\n# Print to verify\n#print(tokenized_dataset)\n#print(tokenized_dataset['train'][0])  # Check the first example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:46:57.160683Z","iopub.execute_input":"2025-04-28T09:46:57.160923Z","iopub.status.idle":"2025-04-28T09:47:39.393748Z","shell.execute_reply.started":"2025-04-28T09:46:57.160896Z","shell.execute_reply":"2025-04-28T09:47:39.393118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset[\"train\"] \ntrain_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:47:39.394676Z","iopub.execute_input":"2025-04-28T09:47:39.394947Z","iopub.status.idle":"2025-04-28T09:47:39.417322Z","shell.execute_reply.started":"2025-04-28T09:47:39.394918Z","shell.execute_reply":"2025-04-28T09:47:39.416663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Evaluation Metrics\n\nWe define a custom `compute_metrics` function to evaluate the model's performance during training.  \nThe metrics calculated are:\n- Accuracy\n- Precision\n- Recall\n- F1 Score\n\nThese metrics give a comprehensive view of model performance, especially for imbalanced datasets like Fake News Detection.\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:47:39.418140Z","iopub.execute_input":"2025-04-28T09:47:39.418414Z","iopub.status.idle":"2025-04-28T09:47:39.432450Z","shell.execute_reply.started":"2025-04-28T09:47:39.418387Z","shell.execute_reply":"2025-04-28T09:47:39.431712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Setup and Execution\n\nWe fine-tune a pre-trained DistilBERT model for binary classification (REAL vs FAKE news).  \nThe training configuration includes:\n- **TrainingArguments** to specify training parameters (batch size, number of epochs, etc.)\n- **Trainer API** from Hugging Face, which simplifies the training loop\n- Real-time training visualization using **TensorBoard** logs\n\nFinally, after training, we save the fine-tuned model for future inference.\n","metadata":{}},{"cell_type":"code","source":"\ndef train_model(tokenized_dataset):\n    \n    #tokenized_dataset = tokenized_dataset[\"train\"] \n    #train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n \n    model = DistilBertForSequenceClassification.from_pretrained(\\\n            \"distilbert-base-uncased\", num_labels=2)\n    # Specify a directory for TensorBoard logs\n    tb_writer = SummaryWriter(log_dir=LOGS_DIR)\n    training_args = TrainingArguments(\n        output_dir=RESULTS_DIR,\n        logging_dir=LOGS_DIR,\n        num_train_epochs=5,  # 1 a quick test run\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_steps=10,\n        eval_strategy =\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        report_to=\"tensorboard\"\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    return model, trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:47:39.433158Z","iopub.execute_input":"2025-04-28T09:47:39.433442Z","iopub.status.idle":"2025-04-28T09:47:39.448100Z","shell.execute_reply.started":"2025-04-28T09:47:39.433421Z","shell.execute_reply":"2025-04-28T09:47:39.447401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, trainer = train_model(tokenized_dataset)\nprint(\"Starting training...\")\nstart_time = time.time()\n    \ntrainer.train()\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Training completed in {elapsed_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:47:39.448812Z","iopub.execute_input":"2025-04-28T09:47:39.448982Z","iopub.status.idle":"2025-04-28T11:03:17.976526Z","shell.execute_reply.started":"2025-04-28T09:47:39.448967Z","shell.execute_reply":"2025-04-28T11:03:17.975910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Launch TensorBoard: \nAfter training starts, we can use the following command to launch TensorBoard and visualize the progress. This will allow you to visualize the loss, accuracy, and other metrics in real-time during training.\nOnce it finishes a few steps, TensorBoard should start showing nice loss/accuracy plots and learning curves.\n\nOnce training is complete, we save the fine-tuned model for future inference, ensuring it is ready for deployment or further evaluation.","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:03:17.977200Z","iopub.execute_input":"2025-04-28T11:03:17.977428Z","iopub.status.idle":"2025-04-28T11:03:24.015252Z","shell.execute_reply.started":"2025-04-28T11:03:17.977410Z","shell.execute_reply":"2025-04-28T11:03:24.014665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate model on eval_dataset\npredictions = trainer.predict(eval_dataset)\n\n# Print evaluation metrics\nmetrics = compute_metrics(predictions)\nprint(\"Evaluation Metrics:\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:03:24.015970Z","iopub.execute_input":"2025-04-28T11:03:24.016282Z","iopub.status.idle":"2025-04-28T11:04:32.790516Z","shell.execute_reply.started":"2025-04-28T11:03:24.016264Z","shell.execute_reply":"2025-04-28T11:04:32.789924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Saving the Fine-Tuned Model\n\nAfter training the model, we save the fine-tuned model to disk in a **ZIP archive** format for easy storage or sharing.  \nThis archive contains the model's weights and configuration, making it portable for use in other environments.\n","metadata":{}},{"cell_type":"code","source":"# Save fine-tuned model\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nmodel.save_pretrained(MODEL_SAVE_DIR)\nshutil.make_archive(\"/kaggle/working/fine_tuned_model\", 'zip', MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:04:32.792476Z","iopub.execute_input":"2025-04-28T11:04:32.792714Z","iopub.status.idle":"2025-04-28T11:04:46.038827Z","shell.execute_reply.started":"2025-04-28T11:04:32.792695Z","shell.execute_reply":"2025-04-28T11:04:46.038069Z"}},"outputs":[],"execution_count":null}]}