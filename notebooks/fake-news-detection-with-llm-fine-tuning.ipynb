{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11547066,"sourceType":"datasetVersion","datasetId":7241323}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n### End-to-End Fine-Tuning on the ISOT Fake News Dataset\n\nFine-tuning a pre-trained transformer model for binary classification (Real vs. Fake News) using Hugging Face `transformers`.\n\n---\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we fine-tune a pre-trained transformer model to perform fake news detection.  \nUsing the ISOT Fake News Dataset, we demonstrate the full end-to-end process, including data loading, preprocessing, tokenization, model training, evaluation, and saving the fine-tuned model.  \nWe leverage the Hugging Face `transformers` library to simplify the workflow and take advantage of powerful state-of-the-art language models.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies\n\nThe following libraries are required for this notebook:\n- `transformers` (for model and training pipeline)\n- `datasets` (for easy dataset handling)\n- `tensorboard` (for training visualization)\n\nIf you are running this notebook for the first time, you may need to install them by uncommenting the commands below.\n","metadata":{}},{"cell_type":"code","source":"#!pip install -q transformers \n#!pip install -q datasets \n#!pip install tensorboard  #TensorBoard is a great option to visualize the training progress in real-time.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:39:52.950655Z","iopub.execute_input":"2025-04-29T09:39:52.950936Z","iopub.status.idle":"2025-04-29T09:39:52.954994Z","shell.execute_reply.started":"2025-04-29T09:39:52.950911Z","shell.execute_reply":"2025-04-29T09:39:52.954304Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Setup and Configuration\n\nWe import necessary libraries and set up the device configuration for training.","metadata":{}},{"cell_type":"markdown","source":"### Libraries\n\nWe import all the necessary libraries for model training, evaluation, and logging.\n","metadata":{}},{"cell_type":"code","source":"# Silence warnings and reduce logging noise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Silence logging output from the Hugging Face transformers library (which can be verbose)\n#from transformers.utils import logging\n#logging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:39:52.955647Z","iopub.execute_input":"2025-04-29T09:39:52.955821Z","iopub.status.idle":"2025-04-29T09:39:52.973434Z","shell.execute_reply.started":"2025-04-29T09:39:52.955806Z","shell.execute_reply":"2025-04-29T09:39:52.972711Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Libraries\nfrom transformers import Trainer, TrainingArguments, pipeline,\\\n                         DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom datasets import Dataset\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\nimport os\nimport time\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:39:52.975284Z","iopub.execute_input":"2025-04-29T09:39:52.975497Z","iopub.status.idle":"2025-04-29T09:40:25.186015Z","shell.execute_reply.started":"2025-04-29T09:39:52.975478Z","shell.execute_reply":"2025-04-29T09:40:25.185418Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 09:40:07.785084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745919608.086681      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745919608.169071      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Paths and Directory Setup\n\nDefine project paths for saving results, logs, and trained models.\n","metadata":{}},{"cell_type":"code","source":"# paths\nPROJECT_ROOT = \"/kaggle/working\"\nDATA_ROOT = \"/kaggle/input/fake-news-dataset-for-llm-fine-tuning/fake_news_detection_Kaggle\"\n\nRESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\nLOGS_DIR = os.path.join(PROJECT_ROOT, \"logs\")\nMODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\", \"fine_tuned_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:40:25.186665Z","iopub.execute_input":"2025-04-29T09:40:25.187216Z","iopub.status.idle":"2025-04-29T09:40:25.191327Z","shell.execute_reply.started":"2025-04-29T09:40:25.187194Z","shell.execute_reply":"2025-04-29T09:40:25.190650Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Quick Check: Using a Pre-trained Model\n\nWe first load a pre-trained DistilBERT model fine-tuned for sentiment analysis to quickly test the text classification workflow.  \n*Note: This model is only used as a placeholder and will later be replaced by our fine-tuned model.*\n","metadata":{}},{"cell_type":"code","source":"#---------------------------------------------------\n# Load the pre-trained model from Hugging Face\n# This will download on first run and cache locally\nclassifier = pipeline(\"text-classification\", \\\n             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n#---------------------------------------------------\ndef predict_with_llm(text):\n    \"\"\"\n    Predict whether the input text is FAKE or REAL using a pre-trained LLM.\n    Note: This model is fine-tuned for sentiment analysis, so this is just a placeholder.\n    \"\"\"\n    result = classifier(text)[0]\n    label = result['label']  # 'POSITIVE' or 'NEGATIVE'\n\n    # TEMPORARY MAPPING\n    if label == 'NEGATIVE':\n        return \"FAKE\"\n    else:\n        return \"REAL\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:40:25.192034Z","iopub.execute_input":"2025-04-29T09:40:25.192292Z","iopub.status.idle":"2025-04-29T09:40:33.879658Z","shell.execute_reply.started":"2025-04-29T09:40:25.192267Z","shell.execute_reply":"2025-04-29T09:40:33.878895Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e7cd7f170b4ef9ad290bfdea2e297e"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55273aec1fa54ce0a6af617ac64e75b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52a80e89a5b42068e03bf99461a3855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b366bb1e23ba4c8c8eddd85e5e85186a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Dataset Preparation\n\nWe load the ISOT Fake News dataset, assign labels (1 for REAL, 0 for FAKE), and combine the true and fake news articles into a single DataFrame.  \nThe dataset is then shuffled, converted to a Hugging Face `Dataset`, and split into training and test sets (80% train / 20% test).\n","metadata":{}},{"cell_type":"code","source":"def prepare_dataset():\n    # Load both parts of ISOT dataset\n    true_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"True.csv\"))\n    fake_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"Fake.csv\"))\n\n    # Add labels\n    true_df[\"label\"] = 1  # REAL\n    fake_df[\"label\"] = 0  # FAKE\n\n    # Keep only the 'text' and 'label' columns\n    true_df = true_df[[\"text\", \"label\"]]\n    fake_df = fake_df[[\"text\", \"label\"]]\n\n    # Combine and shuffle\n    df = pd.concat([true_df, fake_df], ignore_index=True)\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Convert to Hugging Face Dataset and split\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.2)\n\n    return dataset  # A DatasetDict with 'train' and 'test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:40:33.880414Z","iopub.execute_input":"2025-04-29T09:40:33.880628Z","iopub.status.idle":"2025-04-29T09:40:33.886130Z","shell.execute_reply.started":"2025-04-29T09:40:33.880610Z","shell.execute_reply":"2025-04-29T09:40:33.885462Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Tokenization\n\nWe use the pre-trained DistilBERT tokenizer from Hugging Face to tokenize the text data.  \nEach text entry is tokenized with truncation and padding to a maximum length of 512 tokens.  \nThe tokenized dataset is then formatted for PyTorch compatibility, making it ready for model training.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset(dataset):\n    '''\n    Load the tokenizer from Hugging Face.\n    Define a preprocessing function that:\n    Tokenize th|e text field.\n    Applly truncation and padding up to 512 tokens.\n    Appliy this to the full dataset using map.\n    Set the format of the returned dataset for PyTorch\n    '''\n    tokenizer = DistilBertTokenizerFast.from_pretrained(\\\n                 'distilbert-base-uncased')\n    def tokenize_fn(example):\n        return tokenizer(\n            example['text'], \n            padding='max_length', \n            truncation=True, \n            max_length=512\n        )\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    # This part is essential for model training with PyTorch\n    tokenized_dataset.set_format(\n        type='torch',\n        columns=['input_ids', 'attention_mask', 'label']\n    )\n    return tokenized_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:40:33.886894Z","iopub.execute_input":"2025-04-29T09:40:33.887131Z","iopub.status.idle":"2025-04-29T09:40:33.980685Z","shell.execute_reply.started":"2025-04-29T09:40:33.887110Z","shell.execute_reply":"2025-04-29T09:40:33.979908Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Step 1: Prepare the dataset\ndataset = prepare_dataset()  # This returns a Hugging Face DatasetDict\n#print(dataset)  # Should now show DatasetDict with train/test\n\n# Step 2: Tokenize the dataset\ntokenized_dataset, tokenizer = tokenize_dataset(dataset)\n\n# Print to verify\n#print(tokenized_dataset)\n#print(tokenized_dataset['train'][0])  # Check the first example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:40:33.981497Z","iopub.execute_input":"2025-04-29T09:40:33.981704Z","iopub.status.idle":"2025-04-29T09:41:16.363148Z","shell.execute_reply.started":"2025-04-29T09:40:33.981686Z","shell.execute_reply":"2025-04-29T09:41:16.362575Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9cef6a15b14ba3a401876c52651df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0489fd914d6b4bfcb75fae1ea259b610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc6b6f32046e486eb4069f8b4546e23b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad39543fbc2743edadc5957b97474fca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f7008ad9e84c2bac02791810b84743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8980 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57170fb7b762433d80b8b2ba4aa46b91"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset[\"train\"] \ntrain_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:41:16.365290Z","iopub.execute_input":"2025-04-29T09:41:16.365698Z","iopub.status.idle":"2025-04-29T09:41:16.385385Z","shell.execute_reply.started":"2025-04-29T09:41:16.365679Z","shell.execute_reply":"2025-04-29T09:41:16.384731Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Save tokenizer\ntokenizer.save_pretrained(MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:41:16.386051Z","iopub.execute_input":"2025-04-29T09:41:16.386285Z","iopub.status.idle":"2025-04-29T09:41:16.409487Z","shell.execute_reply.started":"2025-04-29T09:41:16.386261Z","shell.execute_reply":"2025-04-29T09:41:16.408775Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/models/fine_tuned_model/tokenizer_config.json',\n '/kaggle/working/models/fine_tuned_model/special_tokens_map.json',\n '/kaggle/working/models/fine_tuned_model/vocab.txt',\n '/kaggle/working/models/fine_tuned_model/added_tokens.json',\n '/kaggle/working/models/fine_tuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Defining Evaluation Metrics\n\nWe define a custom `compute_metrics` function to evaluate the model's performance during training.  \nThe metrics calculated are:\n- Accuracy\n- Precision\n- Recall\n- F1 Score\n\nThese metrics give a comprehensive view of model performance, especially for imbalanced datasets like Fake News Detection.\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:41:16.410227Z","iopub.execute_input":"2025-04-29T09:41:16.410909Z","iopub.status.idle":"2025-04-29T09:41:16.414787Z","shell.execute_reply.started":"2025-04-29T09:41:16.410884Z","shell.execute_reply":"2025-04-29T09:41:16.414098Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Training Setup and Execution\n\nWe fine-tune a pre-trained DistilBERT model for binary classification (REAL vs FAKE news).  \nThe training configuration includes:\n- **TrainingArguments** to specify training parameters (batch size, number of epochs, etc.)\n- **Trainer API** from Hugging Face, which simplifies the training loop\n- Real-time training visualization using **TensorBoard** logs\n\nFinally, after training, we save the fine-tuned model for future inference.\n","metadata":{}},{"cell_type":"code","source":"\ndef train_model(tokenized_dataset):\n    \n    #tokenized_dataset = tokenized_dataset[\"train\"] \n    #train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n \n    model = DistilBertForSequenceClassification.from_pretrained(\\\n            \"distilbert-base-uncased\", num_labels=2)\n    # Specify a directory for TensorBoard logs\n    tb_writer = SummaryWriter(log_dir=LOGS_DIR)\n    training_args = TrainingArguments(\n        output_dir=RESULTS_DIR,\n        logging_dir=LOGS_DIR,\n        num_train_epochs=5,  # 1 a quick test run\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_steps=10,\n        eval_strategy =\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        report_to=\"tensorboard\"\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    return model, trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:41:16.415511Z","iopub.execute_input":"2025-04-29T09:41:16.415766Z","iopub.status.idle":"2025-04-29T09:41:16.431304Z","shell.execute_reply.started":"2025-04-29T09:41:16.415745Z","shell.execute_reply":"2025-04-29T09:41:16.430754Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model, trainer = train_model(tokenized_dataset)\nprint(\"Starting training...\")\nstart_time = time.time()\n    \ntrainer.train()\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Training completed in {elapsed_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:41:16.431997Z","iopub.execute_input":"2025-04-29T09:41:16.432200Z","iopub.status.idle":"2025-04-29T10:51:37.750529Z","shell.execute_reply.started":"2025-04-29T09:41:16.432176Z","shell.execute_reply":"2025-04-29T10:51:37.749901Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb7292b893d4c7fae21451edbdf0004"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4490' max='4490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4490/4490 1:10:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000100</td>\n      <td>0.000549</td>\n      <td>0.999861</td>\n      <td>0.999854</td>\n      <td>1.000000</td>\n      <td>0.999709</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000935</td>\n      <td>0.999861</td>\n      <td>0.999854</td>\n      <td>1.000000</td>\n      <td>0.999709</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000766</td>\n      <td>0.999861</td>\n      <td>0.999854</td>\n      <td>1.000000</td>\n      <td>0.999709</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>0.000161</td>\n      <td>0.999861</td>\n      <td>0.999854</td>\n      <td>0.999709</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>0.000186</td>\n      <td>0.999861</td>\n      <td>0.999854</td>\n      <td>0.999709</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed in 4219.02 seconds\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"#### Launch TensorBoard: \nAfter training starts, we can use the following command to launch TensorBoard and visualize the progress. This will allow you to visualize the loss, accuracy, and other metrics in real-time during training.\nOnce it finishes a few steps, TensorBoard should start showing nice loss/accuracy plots and learning curves.\n\nOnce training is complete, we save the fine-tuned model for future inference, ensuring it is ready for deployment or further evaluation.","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:51:37.751244Z","iopub.execute_input":"2025-04-29T10:51:37.751681Z","iopub.status.idle":"2025-04-29T10:51:44.295562Z","shell.execute_reply.started":"2025-04-29T10:51:37.751661Z","shell.execute_reply":"2025-04-29T10:51:44.294912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate model on eval_dataset\npredictions = trainer.predict(eval_dataset)\n\n# Print evaluation metrics\nmetrics = compute_metrics(predictions)\nprint(\"Evaluation Metrics:\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:51:44.296284Z","iopub.execute_input":"2025-04-29T10:51:44.296517Z","iopub.status.idle":"2025-04-29T10:52:44.234038Z","shell.execute_reply.started":"2025-04-29T10:51:44.296489Z","shell.execute_reply":"2025-04-29T10:52:44.233441Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation Metrics:\naccuracy: 0.9999\nf1: 0.9999\nprecision: 1.0000\nrecall: 0.9997\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"#### Saving the Fine-Tuned Model and Tokenizer\n\nAfter training the model, we save the fine-tuned model to disk in a **ZIP archive** format for easy storage or sharing.  \nThis archive contains the model's weights and configuration, making it portable for use in other environments.\n","metadata":{}},{"cell_type":"code","source":"# Save fine-tuned model\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nmodel.save_pretrained(MODEL_SAVE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:52:44.234825Z","iopub.execute_input":"2025-04-29T10:52:44.235063Z","iopub.status.idle":"2025-04-29T10:52:44.891569Z","shell.execute_reply.started":"2025-04-29T10:52:44.235045Z","shell.execute_reply":"2025-04-29T10:52:44.891012Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/fine_tuned_model\", 'zip', MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T11:42:02.132223Z","iopub.execute_input":"2025-04-29T11:42:02.132875Z","iopub.status.idle":"2025-04-29T11:42:15.653045Z","shell.execute_reply.started":"2025-04-29T11:42:02.132832Z","shell.execute_reply":"2025-04-29T11:42:15.652419Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/fine_tuned_model.zip'"},"metadata":{}}],"execution_count":18}]}