{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11547066,"sourceType":"datasetVersion","datasetId":7241323}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n### End-to-End Fine-Tuning on the ISOT Fake News Dataset\n\nFine-tuning a pre-trained transformer model for binary classification (Real vs. Fake News) using Hugging Face `transformers`.\n\n---\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we fine-tune a pre-trained transformer model to perform fake news detection.  \nUsing the ISOT Fake News Dataset, we demonstrate the full end-to-end process, including data loading, preprocessing, tokenization, model training, evaluation, and saving the fine-tuned model.  \nWe leverage the Hugging Face `transformers` library to simplify the workflow and take advantage of powerful state-of-the-art language models.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies\n\nThe following libraries are required for this notebook:\n- `transformers` (for model and training pipeline)\n- `datasets` (for easy dataset handling)\n- `tensorboard` (for training visualization)\n\nIf you are running this notebook for the first time, you may need to install them by uncommenting the commands below.\n","metadata":{}},{"cell_type":"code","source":"#!pip install -q transformers \n#!pip install -q datasets \n#!pip install tensorboard  #TensorBoard is a great option to visualize the training progress in real-time.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:06:37.068517Z","iopub.execute_input":"2025-04-29T18:06:37.068747Z","iopub.status.idle":"2025-04-29T18:06:37.072983Z","shell.execute_reply.started":"2025-04-29T18:06:37.068726Z","shell.execute_reply":"2025-04-29T18:06:37.071710Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Setup and Configuration\n\nWe import necessary libraries and set up the device configuration for training.","metadata":{}},{"cell_type":"markdown","source":"### Libraries\n\nWe import all the necessary libraries for model training, evaluation, and logging.\n","metadata":{}},{"cell_type":"code","source":"# Silence warnings and reduce logging noise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Silence logging output from the Hugging Face transformers library (which can be verbose)\n#from transformers.utils import logging\n#logging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:06:37.073664Z","iopub.execute_input":"2025-04-29T18:06:37.073911Z","iopub.status.idle":"2025-04-29T18:06:37.090620Z","shell.execute_reply.started":"2025-04-29T18:06:37.073888Z","shell.execute_reply":"2025-04-29T18:06:37.089868Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Libraries\nfrom transformers import Trainer, TrainingArguments, pipeline,\\\n                         DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom datasets import Dataset\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\nimport os\nimport time\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:06:37.092477Z","iopub.execute_input":"2025-04-29T18:06:37.092695Z","iopub.status.idle":"2025-04-29T18:07:05.102572Z","shell.execute_reply.started":"2025-04-29T18:06:37.092673Z","shell.execute_reply":"2025-04-29T18:07:05.101801Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 18:06:49.460963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745950009.673576      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745950009.731517      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Paths and Directory Setup\n\nDefine project paths for saving results, logs, and trained models.\n","metadata":{}},{"cell_type":"code","source":"# paths\nPROJECT_ROOT = \"/kaggle/working\"\nDATA_ROOT = \"/kaggle/input/fake-news-dataset-for-llm-fine-tuning/fake_news_detection_Kaggle\"\n\nRESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\nLOGS_DIR = os.path.join(PROJECT_ROOT, \"logs\")\nMODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\", \"fine_tuned_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:05.103359Z","iopub.execute_input":"2025-04-29T18:07:05.103914Z","iopub.status.idle":"2025-04-29T18:07:05.107978Z","shell.execute_reply.started":"2025-04-29T18:07:05.103892Z","shell.execute_reply":"2025-04-29T18:07:05.107254Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Quick Check: Using a Pre-trained Model\n\nWe first load a pre-trained DistilBERT model fine-tuned for sentiment analysis to quickly test the text classification workflow.  \n*Note: This model is only used as a placeholder and will later be replaced by our fine-tuned model.*\n","metadata":{}},{"cell_type":"code","source":"#---------------------------------------------------\n# Load the pre-trained model from Hugging Face\n# This will download on first run and cache locally\nclassifier = pipeline(\"text-classification\", \\\n             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n#---------------------------------------------------\ndef predict_with_llm(text):\n    \"\"\"\n    Predict whether the input text is FAKE or REAL using a pre-trained LLM.\n    Note: This model is fine-tuned for sentiment analysis, so this is just a placeholder.\n    \"\"\"\n    result = classifier(text)[0]\n    label = result['label']  # 'POSITIVE' or 'NEGATIVE'\n\n    # TEMPORARY MAPPING\n    if label == 'NEGATIVE':\n        return \"FAKE\"\n    else:\n        return \"REAL\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:05.108834Z","iopub.execute_input":"2025-04-29T18:07:05.109159Z","iopub.status.idle":"2025-04-29T18:07:07.831643Z","shell.execute_reply.started":"2025-04-29T18:07:05.109084Z","shell.execute_reply":"2025-04-29T18:07:07.831112Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb7a21ea6334aee87dadb16e32ed5e8"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc055681f228486f93d2f350b0806493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301252a9da19467a8f3e9e4660c0293c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"997c6efe7d634ad1bdbd31e4e2ec5492"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Dataset Preparation\n\nWe load the [ISOT Fake News dataset](https://onlineacademiccommunity.uvic.ca/isot/2022/11/27/fake-news-detection-datasets/), assign labels (1 for REAL, 0 for FAKE), and combine the true and fake news articles into a single DataFrame.  \nThe dataset is then shuffled, converted to a Hugging Face `Dataset`, and split into training and test sets (80% train / 20% test).\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def prepare_dataset():\n    # Load both parts of ISOT dataset\n    true_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"True.csv\"))\n    fake_df = pd.read_csv(os.path.join(DATA_ROOT, \"data\", \"Fake.csv\"))\n\n    # Add labels\n    true_df[\"label\"] = 1  # REAL\n    fake_df[\"label\"] = 0  # FAKE\n\n    # Keep only the 'text' and 'label' columns\n    true_df = true_df[[\"text\", \"label\"]]\n    fake_df = fake_df[[\"text\", \"label\"]]\n\n    # Combine and shuffle\n    df = pd.concat([true_df, fake_df], ignore_index=True)\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Convert to Hugging Face Dataset and split\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.2)\n\n    return dataset  # A DatasetDict with 'train' and 'test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:07.832450Z","iopub.execute_input":"2025-04-29T18:07:07.832725Z","iopub.status.idle":"2025-04-29T18:07:07.837850Z","shell.execute_reply.started":"2025-04-29T18:07:07.832700Z","shell.execute_reply":"2025-04-29T18:07:07.837063Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Tokenization\n\nWe use the pre-trained DistilBERT tokenizer from Hugging Face to tokenize the text data.  \nEach text entry is tokenized with truncation and padding to a maximum length of 512 tokens.  \nThe tokenized dataset is then formatted for PyTorch compatibility, making it ready for model training.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset(dataset):\n    '''\n    Load the tokenizer from Hugging Face.\n    Define a preprocessing function that:\n    Tokenize th|e text field.\n    Applly truncation and padding up to 512 tokens.\n    Appliy this to the full dataset using map.\n    Set the format of the returned dataset for PyTorch\n    '''\n    tokenizer = DistilBertTokenizerFast.from_pretrained(\\\n                 'distilbert-base-uncased')\n    def tokenize_fn(example):\n        return tokenizer(\n            example['text'], \n            padding='max_length', \n            truncation=True, \n            max_length=512\n        )\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    # This part is essential for model training with PyTorch\n    tokenized_dataset.set_format(\n        type='torch',\n        columns=['input_ids', 'attention_mask', 'label']\n    )\n    return tokenized_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:07.838666Z","iopub.execute_input":"2025-04-29T18:07:07.838950Z","iopub.status.idle":"2025-04-29T18:07:07.856509Z","shell.execute_reply.started":"2025-04-29T18:07:07.838927Z","shell.execute_reply":"2025-04-29T18:07:07.855853Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Step 1: Prepare the dataset\ndataset = prepare_dataset()  # This returns a Hugging Face DatasetDict\n#print(dataset)  # Should now show DatasetDict with train/test\n\n# Step 2: Tokenize the dataset\ntokenized_dataset, tokenizer = tokenize_dataset(dataset)\n\n# Print to verify\n#print(tokenized_dataset)\n#print(tokenized_dataset['train'][0])  # Check the first example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:07.857137Z","iopub.execute_input":"2025-04-29T18:07:07.857370Z","iopub.status.idle":"2025-04-29T18:07:49.572428Z","shell.execute_reply.started":"2025-04-29T18:07:07.857345Z","shell.execute_reply":"2025-04-29T18:07:49.571878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"318c7717c17c47a1989f90cbb5b001ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4388d26da56d4d34aa75faedc0d35d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac7edbfbad44bcea43e74965b50808f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aacee7c1961642eb8096d507f8e12bc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c40cece795e4193943f0afb4f68d7e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8980 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c079d8368e4145afd81dc2bdc12b66"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset[\"train\"] \ntrain_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:49.574748Z","iopub.execute_input":"2025-04-29T18:07:49.574976Z","iopub.status.idle":"2025-04-29T18:07:49.596754Z","shell.execute_reply.started":"2025-04-29T18:07:49.574959Z","shell.execute_reply":"2025-04-29T18:07:49.596203Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Save tokenizer\ntokenizer.save_pretrained(MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:49.597408Z","iopub.execute_input":"2025-04-29T18:07:49.597576Z","iopub.status.idle":"2025-04-29T18:07:49.618215Z","shell.execute_reply.started":"2025-04-29T18:07:49.597563Z","shell.execute_reply":"2025-04-29T18:07:49.617547Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/models/fine_tuned_model/tokenizer_config.json',\n '/kaggle/working/models/fine_tuned_model/special_tokens_map.json',\n '/kaggle/working/models/fine_tuned_model/vocab.txt',\n '/kaggle/working/models/fine_tuned_model/added_tokens.json',\n '/kaggle/working/models/fine_tuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Defining Evaluation Metrics\n\nWe define a custom `compute_metrics` function to evaluate the model's performance during training.  \nThe metrics calculated are:\n- Accuracy\n- Precision\n- Recall\n- F1 Score\n\nThese metrics give a comprehensive view of model performance, especially for imbalanced datasets like Fake News Detection.\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:49.618972Z","iopub.execute_input":"2025-04-29T18:07:49.619233Z","iopub.status.idle":"2025-04-29T18:07:49.623345Z","shell.execute_reply.started":"2025-04-29T18:07:49.619211Z","shell.execute_reply":"2025-04-29T18:07:49.622728Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Training Setup and Execution\n\nWe fine-tune a pre-trained DistilBERT model for binary classification (REAL vs FAKE news).  \nThe training configuration includes:\n- **TrainingArguments** to specify training parameters (batch size, number of epochs, etc.)\n- **Trainer API** from Hugging Face, which simplifies the training loop\n- Real-time training visualization using **TensorBoard** logs\n\nFinally, after training, we save the fine-tuned model for future inference.\n","metadata":{}},{"cell_type":"code","source":"\ndef train_model(tokenized_dataset):\n    \n    #tokenized_dataset = tokenized_dataset[\"train\"] \n    #train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n \n    model = DistilBertForSequenceClassification.from_pretrained(\\\n            \"distilbert-base-uncased\", num_labels=2)\n    # Specify a directory for TensorBoard logs\n    tb_writer = SummaryWriter(log_dir=LOGS_DIR)\n    training_args = TrainingArguments(\n        output_dir=RESULTS_DIR,\n        logging_dir=LOGS_DIR,\n        num_train_epochs=10,  # 1 a quick test run\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_steps=10,\n        eval_strategy =\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        report_to=\"tensorboard\"\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    return model, trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:49.624044Z","iopub.execute_input":"2025-04-29T18:07:49.624289Z","iopub.status.idle":"2025-04-29T18:07:49.636109Z","shell.execute_reply.started":"2025-04-29T18:07:49.624267Z","shell.execute_reply":"2025-04-29T18:07:49.635339Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model, trainer = train_model(tokenized_dataset)\nprint(\"Starting training...\")\nstart_time = time.time()\n    \ntrainer.train()\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Training completed in {elapsed_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:07:49.636738Z","iopub.execute_input":"2025-04-29T18:07:49.636947Z","iopub.status.idle":"2025-04-29T20:33:11.469368Z","shell.execute_reply.started":"2025-04-29T18:07:49.636932Z","shell.execute_reply":"2025-04-29T20:33:11.468693Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d602a1f69bb46bd9b156bc6ae7c4ab3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8980' max='8980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8980/8980 2:25:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000100</td>\n      <td>0.001406</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000487</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>0.999711</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000300</td>\n      <td>0.000253</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>0.001081</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>0.001186</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000100</td>\n      <td>0.001140</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000000</td>\n      <td>0.001554</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000000</td>\n      <td>0.001650</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000000</td>\n      <td>0.001624</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0.001648</td>\n      <td>0.999861</td>\n      <td>0.999856</td>\n      <td>1.000000</td>\n      <td>0.999711</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed in 8720.20 seconds\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"#### Launch TensorBoard: \nAfter training starts, we can use the following command to launch TensorBoard and visualize the progress. This will allow you to visualize the loss, accuracy, and other metrics in real-time during training.\nOnce it finishes a few steps, TensorBoard should start showing nice loss/accuracy plots and learning curves.\n\nOnce training is complete, we save the fine-tuned model for future inference, ensuring it is ready for deployment or further evaluation.","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T20:33:11.470142Z","iopub.execute_input":"2025-04-29T20:33:11.470454Z","iopub.status.idle":"2025-04-29T20:33:17.006299Z","shell.execute_reply.started":"2025-04-29T20:33:11.470436Z","shell.execute_reply":"2025-04-29T20:33:17.005679Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate model on eval_dataset\npredictions = trainer.predict(eval_dataset)\n\n# Print evaluation metrics\nmetrics = compute_metrics(predictions)\nprint(\"Evaluation Metrics:\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T20:33:17.007064Z","iopub.execute_input":"2025-04-29T20:33:17.007621Z","iopub.status.idle":"2025-04-29T20:34:20.695755Z","shell.execute_reply.started":"2025-04-29T20:33:17.007597Z","shell.execute_reply":"2025-04-29T20:34:20.695197Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation Metrics:\naccuracy: 1.0000\nf1: 1.0000\nprecision: 1.0000\nrecall: 1.0000\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"#### Saving the Fine-Tuned Model and Tokenizer\n\nAfter training the model, we save the fine-tuned model to disk in a **ZIP archive** format for easy storage or sharing.  \nThis archive contains the model's weights and configuration, making it portable for use in other environments.\n","metadata":{}},{"cell_type":"code","source":"# Save fine-tuned model\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nmodel.save_pretrained(MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T20:34:20.696418Z","iopub.execute_input":"2025-04-29T20:34:20.696627Z","iopub.status.idle":"2025-04-29T20:34:21.347307Z","shell.execute_reply.started":"2025-04-29T20:34:20.696604Z","shell.execute_reply":"2025-04-29T20:34:21.346746Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/fine_tuned_model\", 'zip', MODEL_SAVE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T20:34:21.347956Z","iopub.execute_input":"2025-04-29T20:34:21.348168Z","iopub.status.idle":"2025-04-29T20:34:34.758893Z","shell.execute_reply.started":"2025-04-29T20:34:21.348152Z","shell.execute_reply":"2025-04-29T20:34:34.758130Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/fine_tuned_model.zip'"},"metadata":{}}],"execution_count":17}]}